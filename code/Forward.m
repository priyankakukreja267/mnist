function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_h', and the hidden layer post
% activations in 'act_a'.

% X is a vector of dimensions N X 1. This function should return
% the final softmax output of the network in the variable out, which is of size C X 1. The
% function must also recturn the cell arrays act a and act h, which contain the pre- and
% post-activations of the network on this input sample.

% display('In function Forward.m');

X = X';

nLayers = size(W, 1) + 1;
nHidden = nLayers - 2;
nInputs = size(W{1}, 1);
% pre-act: LC
% post-act:sigmoid of LC

act_a = cell(nLayers-1, 1);
act_h = cell(nLayers-1, 1);

post_act = X; % 1XN

for i = 1:nLayers-2 % Do only for non-output layers, starting from 2nd layer
    wts = W{i};  % wts is a #Li X #L(i+1)

    % Compute LC of inputs
    pre_act = b{i} + (post_act * wts); % returns a 1 X #L(i+1) data points: one for each of the node in Layer (i+1)
    
    % Compute Sigmoid of LC
    post_act = sigmoid(pre_act);
    
    % Append to pre-act    
    act_a{i} = pre_act;
    
    % Append to post-act
    act_h{i} = post_act;
end

% Find output using softmax
pre_act = b{nLayers - 1} + (post_act * W{nLayers - 1});
post_act = exp(pre_act);
post_act = post_act / sum(post_act);

act_a{nLayers - 1} = pre_act;
act_h{nLayers - 1} = post_act;
output = act_h{nLayers - 1}';
end
 
function out = sigmoid(in)
    out = 1 ./ (1.0 + exp(-in));
end
